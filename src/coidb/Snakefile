from snakemake.utils import validate
import os

# this container defines the underlying OS for each job when using the workflow
# with --use-conda --use-singularity
singularity: "docker://continuumio/miniconda3:4.9.2"


# Validate config
validate(config, "config.schema.yaml")

nrows = None
if config["testing"]["nrows"] > 0:
    nrows = config["testing"]["nrows"]


localrules:
    coidb,
    download,
    filter,
    clean,
    format_sintax,
    format_dada2,
    subseq,
    translate_db,
    download_hmm,
    extract_hmm_subseq,
    trim_subseq_aln,


wildcard_constraints:
    textfile="occurrences.txt|dna.txt|Taxon.tsv",
    zipfile="bold.zip|backbone.zip",


textfile_dict = {
    "Taxon.tsv": "backbone.zip",
    "occurrences.txt": "bold.zip",
    "dna.txt": "bold.zip",
}


rule coidb:
    input:
        expand("bold_clustered.{w}.fasta", w=["assignTaxonomy", "addSpecies", "sintax"]),


rule download_zipfile:
    """
    Download zipfile with database sequences + info
    """
    output:
        "{zipfile}",
    log:
        "logs/download.{zipfile}.log",
    params:
        url=lambda wildcards: config["database"][wildcards.zipfile],
    shell:
        """
        curl -L -o $TMPDIR/{wildcards.zipfile} {params.url} > {log} 2>&1
        mv $TMPDIR/{wildcards.zipfile} {output[0]}
        """


rule download:
    input:
        textfile_dict.values(),


rule extract_zipfile:
    input:
        lambda wildcards: textfile_dict[wildcards.textfile],
    output:
        "{textfile}",
    log:
        "logs/extract.{textfile}.log",
    shell:
        """
        f=$(unzip -l {input[0]} | grep -w {output[0]} | rev | cut -f1 -d ' ' | rev)
        unzip -j -o -d . {input[0]} $f >> {log} 2>&1
        """


rule extract:
    input:
        textfile_dict.keys(),


rule filter_data:
    """
    Filter the BOLD data to genes and taxa of interest
    
    This also keeps only records with BOLD: ids
    """
    input:
        "occurrences.txt",
        "dna.txt",
        "Taxon.tsv",
    output:
        info="bold_info_filtered.tsv",
        fasta="bold.fasta",
    log:
        "bold_info_non-unique-taxa.txt",
    params:
        genes=config["database"]["gene"],
        filter_taxa=config["database"]["taxa"],
        filter_rank=config["database"]["rank"],
        ranks=config["database"]["ranks"],
        tmpf="$TMPDIR/bold_filtered.fasta",
        nrows=nrows,
    script:
        "scripts/common.py"


rule remove_non_standard:
    input:
        "bold.fasta",
    output:
        "bold_filtered.fasta",
    log:
        "logs/remove_non_standard.log",
    params:
        tmpfile="$TMPDIR/bold_seqkit_cleaned.fasta",
        ids="$TMPDIR/bold_non_standard_ids.txt",
        fastafile="$TMPDIR/bold_filtered.fasta",
    shell:
        """
        exec &> {log} 
        # Remove gap characters, then remove leading and trailing 'N'
        seqkit seq -g {input} | seqkit replace -s -r "" -p "N+$" | seqkit replace -s -r "" -p "^N+" > {params.tmpfile}
        # Now remove ids still containing non standard DNA chars
        seqkit grep -s -r -p "[^ACGTacgt]+" {params.tmpfile} | seqkit seq -i | grep ">" | sed 's/>//g' > {params.ids}
        seqkit grep -v -f {params.ids} {params.tmpfile} > {params.fastafile}
        mv {params.fastafile} {output[0]}
        seqkit stats {input[0]} {params.tmpfile} {output[0]}
        """


rule filter:
    input:
        "bold_info_filtered.tsv",
        "bold_filtered.fasta",


rule cluster:
    """
    Cluster the filtered fasta file using vsearch
    """
    input:
        fasta="bold_filtered.fasta",
    output:
        fasta="bold_clustered.fasta",
    log:
        "logs/bold/cluster.log",
    threads: 20
    resources:
        runtime=lambda wildcards, attempt: attempt**2 * 60 * 10,
    params:
        pid=config["database"]["pid"],
    shell:
        """
        cluster_bold.py --threads {threads} --pid {params.pid} \
            {input.fasta} {output.fasta} > {log} 2>&1 
        """


rule clean:
    """
    Cleans headers of sequences in clustered fasta file
    """
    input:
        fasta="bold_clustered.fasta",
    output:
        fasta="bold_clustered_cleaned.fasta",
    script:
        "scripts/common.py"


rule format_dada2:
    """
    Formats the clustered fasta file into DADA2-ready files
    """
    input:
        fasta="bold_clustered_cleaned.fasta",
        info="bold_info_filtered.tsv",
    output:
        assignTaxaFasta="bold_clustered.assignTaxonomy.fasta",
        addSpeciesFasta="bold_clustered.addSpecies.fasta",
    params:
        ranks=config["database"]["ranks"],
    script:
        "scripts/common.py"


rule format_sintax:
    input:
        fasta="bold_clustered_cleaned.fasta",
        info="bold_info_filtered.tsv",
    output:
        fasta="bold_clustered.sintax.fasta",
    log:
        "logs/bold/format_sintax.log",
    params:
        ranks=config["sintax"]["ranks"],
        replace=config["sintax"]["replace_ranks"],
    shell:
        """
        format_sintax.py {input.fasta} {input.info} {output.fasta} --ranks {params.ranks} --replace_rank {params.replace} 2>{log}
        """

## subseq pipeline

splits=[f'{x:03d}' for x in list(range(1,1001))]

rule translate_db:
    """
    Translate each reference sequence in all six reading frames and choose the longest
    """
    output:
        faa = "subseq/bold_clustered.faa",
        coord = "subseq/bold_clustered.coord"
    input:
        fasta=rules.format_sintax.output.fasta
    log:
        "logs/translate_db.log"
    shell:
        """
        translate.py --coord {output.coord} {input.fasta} >{output.faa} 2>{log}
        """

rule split_translations:
    """
    Split translated aa sequence file into chunks
    """
    output:
        expand("subseq/_splits/split{split}.faa", split=splits)
    input:
        rules.translate_db.output.faa,
    log:
        "logs/split_translations.log"
    params:
        outdir=lambda wildcards, output: os.path.dirname(output[0]),
        splits=len(splits),
    resources:
        runtime = 120,
    threads: 4
    shell:
        """
        seqkit split2 -O {params.outdir} -j {threads} -p {params.splits} --by-part-prefix split -1 {input} >{log} 2>&1
        """

rule download_hmm:
    """
    Download COX1 HMM from Interpro
    """
    output:
        temp("PF00115.hmm")
    shell:
        """
        curl -L  "https://www.ebi.ac.uk/interpro/wwwapi//entry/pfam/PF00115?annotation=hmm" | gunzip -c > {output}
        """

rule hmmsearch:
    output:
        "subseq/_hmmsearch/{split}.out"
    input:
        split="subseq/_splits/split{split}.faa",
        hmm=rules.download_hmm.output[0]
    log:
        "logs/hmmsearch/{split}.log"
    threads: 4
    resources:
        runtime = 60,
    shell:
        """
        hmmsearch --domtblout {output} --cut_tc --cpu {threads} {input.hmm} {input.split} > /dev/null 2>{log}
        """

rule extract_hmm_subseq:
    output:
        "subseq/_subseq/{split}.fasta"
    input:
        hmmout=rules.hmmsearch.output[0],
        dnafile=rules.format_sintax.output.fasta,
        coord=rules.translate_db.output.coord
    log:
        "logs/extract_hmm_subseq/{split}.log"
    params:
        min_len=lambda wildcards: config["subseq"]["subseq_min_len"],
        hmm_from=lambda wildcards: config["subseq"]["hmm_from"],
        hmm_to=lambda wildcards: config["subseq"]["hmm_to"],
    shell:
        """
        extract_hmm_subseq.py -m {params.min_len} --hmm_from {params.hmm_from} \
            --hmm_to {params.hmm_to} {input.hmmout} {input.coord} {input.dnafile} > {output} 2>{log}
        """

def get_asv_seq(wildcards):
    if "asv_seq" in config["subseq"].keys():
        return ">ASV\n" + config["subseq"]["asv_seq"] + "\n"
    else:
        return ""

rule align_subseqs:
    output:
        "subseq/_aln/{split}.aln"
    input:
        rules.extract_hmm_subseq.output[0]
    log:
        "logs/align_subseqs/{split}.log"
    threads: 10
    resources:
        runtime = 30,
    params:
        asv_seq = lambda wildcards: get_asv_seq(wildcards),
        tmpdir = "$TMPDIR/clustalo/{split}",
        out = "$TMPDIR/clustalo/subseqs.aln"
    shell:
        """
        mkdir -p {params.tmpdir}
        echo -e "{params.asv_seq}" > {params.tmpdir}/input.fasta
        cat {input} >> {params.tmpdir}/input.fasta
        clustalo -i {params.tmpdir}/input.fasta -o {params.out} --outfmt=a2m -t DNA --threads={threads} -v > {log} 2>&1
        mv {params.out} {output}
        rm -rf {params.tmpdir}
        """

rule trim_subseq_aln:
    """
    Trim alignment to match only the columns of the ASV sequence in the alignment
    """
    output:
        "subseq/_trimmed/{split}.fasta"
    input:
        rules.align_subseqs.output[0]
    log:
        "logs/trim_subseq_aln/{split}.log"
    params:
        min_len=300,
    shell:
        """
        trim_aln.py -m {params.min_len} {input} > {output} 2>{log}
        """

rule subseq:
    output:
        "bold_clustered.subseqs.fasta"
    input:
        expand("subseq/_trimmed/{split}.fasta", split=splits),
    log:
        "logs/gather_subseqs.log"
    shell:
        """
        cat {input} > {output} 2>{log}
        """